name: Scrape Sites

on:
  schedule:
    - cron: '0 0 * * *'  # Run daily at midnight UTC
  workflow_dispatch:      # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    # Grant permissions for the GITHUB_TOKEN to push changes
    permissions:
      contents: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18' # Or your preferred Node.js version

    # Optional but recommended: Install a robust CSV parser like Miller
    - name: Install Miller (mlr)
      run: |
        sudo apt-get update
        sudo apt-get install -y miller

    # Assumes you create scrape_script.sh (see below)
    - name: Make scrape script executable
      run: chmod +x ./scrape_script.sh # Make sure the script is in your repo root or adjust path

    - name: Scrape sites using script
      run: ./scrape_script.sh # Execute the external script

    - name: Commit and push if changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        # Check if the specific directory has changes
        if ! git diff --quiet scraped_sites/; then
          git add scraped_sites/
          git commit -m "Update scraped sites"
          git push
        else
          echo "No changes detected in scraped_sites."
        fi

    - name: Run local scrape script
      run: bash scrape_local.sh